# 5. Ollama for AI model serving

Date: 2025-07-04

Status: draft

Author: Oliver Schlüter

## Problem

We need a lightweight, developer-friendly solution to serve large language models (LLMs). The system should:

- Be simple to deploy and integrate.
- Work well in offline, self-hosted setups to maintain data privacy.
- Support rapid iteration and development without the operational overhead of traditional ML model servers.
- Provide a stable API for programmatic access to models.
- Avoid unnecessary operational or licensing costs.

Existing options like OpenAI’s API or full ML model servers are either cloud-dependent, costly, or too complex for our current needs.

## Decision

We will use Ollama for serving AI models in development and lightweight production environments.

Ollama provides a simple CLI and HTTP API for running and interacting with models like llama, Mistral, Gemma, etc. It allows us to:
- Use Docker to containerize models for consistent CI/CD and deployment.
- Easily switch between different models or versions via ollama pull and ollama run.
- Run quantized models efficiently, even on resource-constrained machines.
- Operate completely for free, without licensing costs or API usage fees.

Ollama aligns well with our requirements for ease of use, local-first design, fast iteration — and zero runtime cost.

## Consequences

Positive
- Ease of Use: Quick model setup with minimal configuration.
- Developer Efficiency: Developers can test and iterate on LLM features locally without cloud dependencies.
- Privacy: Sensitive data stays local, avoiding external API usage.
- Portability: Docker support enables consistent environments across teams and deployment stages.
- Free of Cost: No API usage fees or commercial licenses required.
- Fast Prototyping: Enables short feedback loops and experimentation.

Negative
- Limited Flexibility: Less customizable than full-serving solutions.
- Scalability Constraints: May not be suitable for high-concurrency, production-grade traffic.
- Ecosystem Dependency: Tied to Ollama’s format and roadmap; limited support for custom models outside their ecosystem.

## References

- [Ollama](https://ollama.com/)
